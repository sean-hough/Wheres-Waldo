{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Some hyperparameters\n",
    "training_iters = 10000 \n",
    "learning_rate = 0.0001 \n",
    "batch_size = 16\n",
    "image_size = 64\n",
    "num_channels = 3\n",
    "num_classes = 2\n",
    "p = [0.50, 0.50]\n",
    "#The Model Itself\n",
    "def cnn_model(in_data):\n",
    "    bn = tf.layers.batch_normalization(in_data)\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs=bn, filters=64, kernel_size= 5) \n",
    "    pool1 = tf.layers.MaxPooling2D(pool_size=3, strides=2)(conv1)\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(inputs=norm1, filters=64, kernel_size= 3) \n",
    "    pool2 = tf.layers.MaxPooling2D(pool_size=3, strides=2)(conv2)\n",
    "    flat = tf.layers.Flatten()(pool2)\n",
    "    \n",
    "    fc1 = tf.layers.dense(flat, 384)\n",
    "    fc2 = tf.layers.dense(fc1, 192)\n",
    "    output = tf.layers.dense(fc2,num_classes)\n",
    "    return output\n",
    "\n",
    "\n",
    "#Helper function for parsing filenames into images\n",
    "def parse_function(filename, label):\n",
    "    with tf.name_scope(\"parse_function\"):\n",
    "        image_string = tf.read_file(filename)\n",
    "\n",
    "        # Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "        image = tf.image.decode_jpeg(image_string)\n",
    "        image = tf.image.random_crop(image, [image_size,image_size,num_channels])\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        # We know that each image in this dataset is 64 x 64 pixels + has 3 color channels.\n",
    "        #image.set_shape([image_size, image_size, num_channels])\n",
    "        return image, label\n",
    "\n",
    "#Helper function for artifically increasing variety of training data\n",
    "def train_preprocess(image, label):\n",
    "    with tf.name_scope(\"train_preprocess\"):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_jpeg_quality(image, 90, 100)\n",
    "        image = tf.image.random_saturation(image, 0.75, 1.25)\n",
    "        image = tf.image.random_contrast(image, 0.75, 1.25)\n",
    "        image = tf.image.random_brightness(image, 1.25)\n",
    "        image.set_shape([image_size, image_size, num_channels])\n",
    "        return image, label\n",
    "\n",
    "#Helper function for fixing class imbalance\n",
    "def random_choice(p):\n",
    "    choice = tf.multinomial(tf.log([p]), 1)\n",
    "    return tf.cast(tf.squeeze(choice), tf.int64)\n",
    "    \n",
    "#Reset graph so tensorboard doesn't kill itself\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Make directory to write log data to\n",
    "UID =  str(datetime.datetime.now()).replace(' ', '_').replace(':', '_').replace('.', '_')\n",
    "logPath = \"./logs/Waldo2\" + UID\n",
    "os.mkdir(logPath)\n",
    "\n",
    "#Make paths to pull data from\n",
    "notwaldoPath = os.path.join(\"Training_Images\", \"NotWaldo\")\n",
    "waldoPath = os.path.join(\"Training_Images\", \"Waldo\")\n",
    "\n",
    "#Collect filesnames and labels for samples\n",
    "notwaldo_filenames = [os.path.join(notwaldoPath,filename) for filename in os.listdir(notwaldoPath)]\n",
    "notwaldo_labels = [(1,0) for filename in os.listdir(notwaldoPath)]\n",
    "waldo_filenames = [os.path.join(waldoPath,filename) for filename in os.listdir(waldoPath)]\n",
    "waldo_labels = [(0,1) for filename in os.listdir(waldoPath)]\n",
    "\n",
    "#Make two datasets : one with waldo one without\n",
    "waldo_set = tf.data.Dataset.from_tensor_slices( (waldo_filenames, waldo_labels) )\n",
    "waldo_set = waldo_set.shuffle(len(waldo_filenames)).repeat()\n",
    "notwaldo_set = tf.data.Dataset.from_tensor_slices( (notwaldo_filenames, notwaldo_labels) )\n",
    "notwaldo_set = notwaldo_set.shuffle( len(notwaldo_filenames) ).repeat()\n",
    "\n",
    "#Interleave those two datasets together into the training dataset\n",
    "datasets = [notwaldo_set, waldo_set]\n",
    "choice_dataset = tf.data.Dataset.from_tensors([0])\n",
    "choice_dataset = choice_dataset.map(lambda z: random_choice(p)).repeat()\n",
    "dataset = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=4)\n",
    "dataset = dataset.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "#Make the iterator\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(dataset)\n",
    "\n",
    "#define all the learny things\n",
    "logits = cnn_model(next_element[0])\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels = next_element[1], logits = logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "prediction = tf.argmax(logits, 1)\n",
    "equality = tf.equal(prediction, tf.argmax(next_element[1],1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "\n",
    "def train():\n",
    "    epochs = training_iters\n",
    "    with tf.Session() as sess:\n",
    "        #Collect literally all the data\n",
    "        writer = tf.summary.FileWriter(logPath)\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        tf.summary.scalar('Softmax_Cross_Entropy_Loss', loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.histogram('Predictions', prediction)\n",
    "        tf.summary.histogram(\"BN_gamma0\", sess.graph.get_tensor_by_name('batch_normalization/gamma:0'))\n",
    "        tf.summary.histogram(\"BN_beta0\", sess.graph.get_tensor_by_name('batch_normalization/beta:0'))\n",
    "        tf.summary.histogram(\"conv_kernel0\", sess.graph.get_tensor_by_name('conv2d/kernel:0'))\n",
    "        tf.summary.histogram(\"conv_bias0\", sess.graph.get_tensor_by_name('conv2d/bias:0'))\n",
    "        tf.summary.histogram(\"conv_kernel1\", sess.graph.get_tensor_by_name('conv2d_1/kernel:0'))\n",
    "        tf.summary.histogram(\"conv_bias1\", sess.graph.get_tensor_by_name('conv2d_1/bias:0'))\n",
    "        tf.summary.histogram(\"fc_kernel0\", sess.graph.get_tensor_by_name('dense/kernel:0'))\n",
    "        tf.summary.histogram(\"fc_bias0\", sess.graph.get_tensor_by_name('dense/bias:0'))\n",
    "        tf.summary.histogram(\"fc_kernel1\", sess.graph.get_tensor_by_name('dense_1/kernel:0'))\n",
    "        tf.summary.histogram(\"fc_bias1\", sess.graph.get_tensor_by_name('dense_1/bias:0'))\n",
    "        tf.summary.histogram(\"fc_kernel2\", sess.graph.get_tensor_by_name('dense_2/kernel:0'))\n",
    "        tf.summary.histogram(\"fc_bias2\", sess.graph.get_tensor_by_name('dense_2/bias:0'))\n",
    "        tf.summary.image(\"input_data\", next_element[0])    \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        \n",
    "        sess.run(training_init_op)\n",
    "        print(\"Beginning Training\")\n",
    "        for i in range(epochs):\n",
    "            l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "            if i % 20 == 0:\n",
    "                #print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "                merged = tf.summary.merge_all()\n",
    "                s = sess.run(merged)\n",
    "                writer.add_summary(s, i)\n",
    "train()\n",
    "print(\"Final Accuracy:, Testing Accuracy:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
